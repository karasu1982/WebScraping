{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXIZaEc+/5GA/GAjxTZvxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karasu1982/WebScraping/blob/main/WebScraping(NAVITIME).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 環境設定"
      ],
      "metadata": {
        "id": "AnhrTIXeQLPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 標準ライブラリ\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import time\n",
        "import math\n",
        "\n",
        "# スクレイピング用ライブラリ\n",
        "import httplib2\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "EkL8bVd4QNGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 関数定義"
      ],
      "metadata": {
        "id": "jnj90W70Drew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 文字列置換ロジック\n",
        "r = re.compile( '(%s.*%s)' % (\"\\(\",\"\\)\"), flags=re.DOTALL)"
      ],
      "metadata": {
        "id": "L-dHI2OxD_iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrapingNAVITIME(list1, list2):\n",
        "\n",
        "  base_url = \"https://www.navitime.co.jp/category/{chain}/{pref}\"\n",
        "  headers = {'User-Agent':'Mozilla/5.0'}\n",
        "\n",
        "  pref_list = []\n",
        "  chain_list = []\n",
        "  store_list = []\n",
        "  address_list = []\n",
        "\n",
        "  # class名\n",
        "  ## 市区町村別URLを表すclass\n",
        "  class_address_list=\"address-list-item-link\"\n",
        "\n",
        "  ## 店舗名・住所を表すclass\n",
        "  class_spot_name = \"spot-name\" # 店舗名\n",
        "  class_spot_detail = \"spot-detail-section\" # 店舗情報\n",
        "  class_address = \"spot-detail-value-text\" # 店舗住所の内容（その中の１番目に住所がある）\n",
        "\n",
        "  # 1ページの件数表示\n",
        "  n_pages=15\n",
        "\n",
        "  for l1, l2 in zip(list1, list2):\n",
        "      for i in range(47):\n",
        "      #for i in range(1):\n",
        "          pref = '{0:02d}'.format(i+1)\n",
        "\n",
        "          print(pref)\n",
        "          url1 = []\n",
        "          nums = []\n",
        "\n",
        "          # 第一階層：市区町村URL取得\n",
        "          print('第一階層：' + dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "          h = httplib2.Http()\n",
        "          head, body = h.request(uri = base_url.format(chain=l1, pref = pref))\n",
        "          soup = bs(body, 'html.parser')\n",
        "          b = soup.body\n",
        "\n",
        "          elems=b.find_all(class_=class_address_list)\n",
        "\n",
        "          for elem in elems:\n",
        "              try:\n",
        "                  url = elem.get('href')\n",
        "                  url1.append(url)\n",
        "\n",
        "                  num = r.search(elem.text).group(0).replace(\"(\",\"\").replace(\")\",\"\")\n",
        "                  nums.append(num)\n",
        "              except:\n",
        "                  pass\n",
        "\n",
        "          # 第二階層：店舗名・住所取得\n",
        "          print('第二階層：' + dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "          timer1 = dt.datetime.now()\n",
        "          counter = 0\n",
        "\n",
        "          for u, num in zip(url1,nums):\n",
        "              max_page=math.ceil(int(num)/n_pages)\n",
        "\n",
        "              for i in range(max_page):\n",
        "                  #print(u+\"?page=\"+str(i+1))\n",
        "                  h = httplib2.Http()\n",
        "                  head, body = h.request(uri = u+\"?page=\"+str(i+1), headers=headers)\n",
        "                  soup = bs(body, 'html.parser')\n",
        "                  b = soup.body\n",
        "\n",
        "                  stores = b.find_all(class_=class_spot_name)\n",
        "\n",
        "                  # 同じclass名で住所や電話番号が入っているため、各店舗の最初のclass名のセルのみを取得\n",
        "                  addresses = b.select(f'dl[class=\"{class_spot_detail}\"]')\n",
        "                  addresses2 = [] \n",
        "\n",
        "                  for add in addresses:\n",
        "                    addresses2.append(add.find(class_=class_address))\n",
        "                    \n",
        "                  if len(stores)!=len(addresses2):\n",
        "                    print(l2+\"は、店舗と住所がズレている\")\n",
        "\n",
        "                  for store, address in zip(stores, addresses2):\n",
        "                    pref_list.append(pref)\n",
        "                    chain_list.append(l2)\n",
        "                    store_list.append(store.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\"))\n",
        "                    address_list.append(address.text.replace(\"\\n\",\"\"))\n",
        "\n",
        "                  time.sleep(1)\n",
        "\n",
        "              timer2 = dt.datetime.now()\n",
        "              counter += 1\n",
        "              print('    ' + str(counter) + ' / ' + str(len(url1)) + ' : ' + str((timer2 - timer1).seconds) + 'sec')\n",
        "\n",
        "  return store_list, address_list, pref_list, chain_list"
      ],
      "metadata": {
        "id": "bqgb-ue5D3hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 処理実行"
      ],
      "metadata": {
        "id": "yuxjZTKUQR6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list1はchainのIDをリスト化、list2はその名称\n",
        "list1 = [\"0201001001\",\"0201001009\", \"0201001006\", \"0201001005\", \"0201001004\", \"0201001011\", \"0201001003\"]\n",
        "list2 = [\"セブンイレブン\", \"ローソン\", \"ファミリーマート\", \"ミニストップ\", \"デイリーヤマザキ\", \"セイコーマート\", \"その他コンビニ\"]\n",
        "\n",
        "store_list, address_list, pref_list, chain_list = scrapingNAVITIME(list1, list2)\n",
        "\n",
        "df_output = pd.DataFrame({\"store_name\":store_list,\"address\":address_list,\"pref\":pref_list,\"chain\":chain_list})\n",
        "df_output.head()"
      ],
      "metadata": {
        "id": "Ie-gh4yzEeXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}